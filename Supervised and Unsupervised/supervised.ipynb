{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311d7a6a",
   "metadata": {},
   "source": [
    "**Supervised Learning Models: Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1296b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ“š Imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e034d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UK shape: (229782, 21)\n",
      "US shape: (4159, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0k/whv99z5n5rnb1xp8n07krpcw0000gn/T/ipykernel_1729/3102222315.py:2: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  UK_data = pd.read_csv('/Users/mahnooriqbal/COMP702 Project/ML-AI-Risk-Analysis-AV-Data-/Datasets/UK-cleaned_data.csv')\n"
     ]
    }
   ],
   "source": [
    "# Read UK raw data for labelling the cols\n",
    "UK_data = pd.read_csv('/Users/mahnooriqbal/COMP702 Project/ML-AI-Risk-Analysis-AV-Data-/Datasets/UK-cleaned_data.csv')\n",
    "# Read US scaled data\n",
    "US_data = pd.read_csv('/Users/mahnooriqbal/COMP702 Project/ML-AI-Risk-Analysis-AV-Data-/Datasets/US_imputed_raw_data.csv')\n",
    "\n",
    "# Print shape of each DataFrame\n",
    "UK_data = UK_data.drop(['longitude', 'latitude'], axis=1)\n",
    "\n",
    "\n",
    "print(\"UK shape:\", UK_data.shape)\n",
    "print(\"US shape:\", US_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "865581d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Minor' 'Serious' 'Fatality']\n",
      "['No Injuries Reported' 'Minor' 'Serious' 'Moderate' 'Fatality']\n"
     ]
    }
   ],
   "source": [
    "# ðŸ§¹ Make sure both have a binary target column called 'severity'\n",
    "# Assume: severity = 1 if fatal/severe, 0 otherwise\n",
    "# Make sure to preprocess these before use (drop NA, encode categoricals, etc.)\n",
    "\n",
    "# UK_df_scaled.head()\n",
    "# US_df_scaled.head()\n",
    "# we have to rescale these as we must merge the accidents into binary\n",
    "print(UK_data['Highest Injury Severity Alleged'].unique())\n",
    "print(US_data['Highest Injury Severity Alleged'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0027821c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# For UK Dataset\n",
    "UK_data['Highest Injury Severity Alleged'] = UK_data['Highest Injury Severity Alleged'].apply(lambda x: 0 if x == 'Minor' else 1)\n",
    "\n",
    "# For US Dataset\n",
    "def map_us_severity(x):\n",
    "    if x in ['No Injuries Reported', 'Minor']:\n",
    "        return 0\n",
    "    else:  # Serious or Fatality\n",
    "        return 1\n",
    "\n",
    "US_data['Highest Injury Severity Alleged'] = US_data['Highest Injury Severity Alleged'].apply(map_us_severity)\n",
    "# note uk data has no 'no induries hence we will binary minor and no injuries together \n",
    "# and serious, moderarte and fatality together as minor/no injury and serious/fatalities\n",
    "print(UK_data['Highest Injury Severity Alleged'].unique())\n",
    "print(US_data['Highest Injury Severity Alleged'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a36734f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US Data Shape: (4159, 23)\n",
      "UK Data Shape: (229782, 23)\n",
      "\n",
      "UK Preview:\n",
      "+---+-----------+----------------+------+-------+------------+---------------+----------------------------+-----------------------+------+-------+--------------+-----------------+--------------------------+----------+------------+---------------------------------+-----------------------+---------+-----------------+---------+---------------+----------------+--------------------+\n",
      "|   | Report ID | Report Version | Make | Model | Model Year | ADS Equipped? | Automation System Engaged? | Incident Time (24:00) | City | State | Roadway Type | Roadway Surface | Posted Speed Limit (MPH) | Lighting | Crash With | Highest Injury Severity Alleged | SV Pre-Crash Movement | Weather | SV Contact Area | Country | Incident_Year | Incident_Month | Incident_DayOfWeek |\n",
      "+---+-----------+----------------+------+-------+------------+---------------+----------------------------+-----------------------+------+-------+--------------+-----------------+--------------------------+----------+------------+---------------------------------+-----------------------+---------+-----------------+---------+---------------+----------------+--------------------+\n",
      "| 0 |     0     |     29261      |  50  |  441  |     48     |       0       |             0              |         4500          | 777  |   0   |      3       |        0        |            1             |    0     |     6      |                0                |          14           |    0    |        2        |    1    |     2019      |       1        |         1          |\n",
      "| 1 |     1     |     29262      |  17  |  228  |     48     |       0       |             0              |         15000         | 2016 |   0   |      0       |        0        |            2             |    0     |     8      |                1                |           3           |    0    |        2        |    1    |     2019      |       1        |         1          |\n",
      "| 2 |     2     |     29263      |  30  |  91   |     51     |       0       |             0              |         54420         | 3557 |   0   |      3       |        0        |            0             |    4     |     6      |                0                |           3           |    0    |        2        |    1    |     2019      |       1        |         2          |\n",
      "| 3 |     3     |     29264      |  52  |  75   |     38     |       0       |             0              |         76800         | 1597 |   0   |      3       |        0        |            0             |    0     |     6      |                0                |           3           |    0    |        2        |    1    |     2019      |       1        |         1          |\n",
      "| 4 |     4     |     29265      |  2   |  50   |     50     |       0       |             0              |         30000         | 3490 |   0   |      0       |        0        |            1             |    4     |     6      |                1                |           3           |    0    |        2        |    1    |     2019      |       1        |         3          |\n",
      "+---+-----------+----------------+------+-------+------------+---------------+----------------------------+-----------------------+------+-------+--------------+-----------------+--------------------------+----------+------------+---------------------------------+-----------------------+---------+-----------------+---------+---------------+----------------+--------------------+\n",
      "\n",
      "US Preview:\n",
      "+---+-----------+----------------+------+-------+------------+---------------+----------------------------+-----------------------+------+-------+--------------+-----------------+--------------------------+----------+------------+---------------------------------+-----------------------+---------+-----------------+---------+---------------+----------------+--------------------+\n",
      "|   | Report ID | Report Version | Make | Model | Model Year | ADS Equipped? | Automation System Engaged? | Incident Time (24:00) | City | State | Roadway Type | Roadway Surface | Posted Speed Limit (MPH) | Lighting | Crash With | Highest Injury Severity Alleged | SV Pre-Crash Movement | Weather | SV Contact Area | Country | Incident_Year | Incident_Month | Incident_DayOfWeek |\n",
      "+---+-----------+----------------+------+-------+------------+---------------+----------------------------+-----------------------+------+-------+--------------+-----------------+--------------------------+----------+------------+---------------------------------+-----------------------+---------+-----------------+---------+---------------+----------------+--------------------+\n",
      "| 0 |     0     |       3        |  24  |  85   |     8      |       0       |             0              |         71820         | 689  |  37   |      4       |        0        |            4             |    1     |     8      |                0                |          13           |   16    |       32        |    0    |     2025      |       11       |         4          |\n",
      "| 1 |     1     |       0        |  24  |  85   |     8      |       1       |             1              |         71820         | 690  |  37   |      4       |        0        |            4             |    1     |     8      |                0                |          13           |   16    |       32        |    0    |     2021      |       11       |         2          |\n",
      "| 2 |     2     |       0        |  24  |  87   |     7      |       1       |             1              |         57120         | 1210 |   2   |      0       |        0        |            13            |    4     |     8      |                0                |          13           |    0    |       32        |    0    |     2022      |       4        |         6          |\n",
      "| 3 |     3     |       0        |  24  |  85   |     8      |       0       |             0              |         54540         | 1331 |  19   |      0       |        0        |            13            |    4     |     10     |                0                |          13           |    0    |       32        |    0    |     2025      |       10       |         2          |\n",
      "| 4 |     4     |       1        |  24  |  87   |     8      |       0       |             0              |         30000         | 790  |  46   |      0       |        0        |            14            |    4     |     8      |                0                |          13           |    0    |       12        |    0    |     2025      |       12       |         0          |\n",
      "+---+-----------+----------------+------+-------+------------+---------------+----------------------------+-----------------------+------+-------+--------------+-----------------+--------------------------+----------+------------+---------------------------------+-----------------------+---------+-----------------+---------+---------------+----------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "# reencode and scale \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "# Define your columns (fixed syntax errors and duplicates)\n",
    "categorical_columns = [\n",
    "    'Highest Injury Severity Alleged', 'Roadway Type', 'Lighting',\n",
    "    'Weather', 'Roadway Surface', 'City', 'State',\n",
    "    'Make', 'Model', 'Model Year', 'ADS Equipped?', 'Automation System Engaged?',\n",
    "    'Posted Speed Limit (MPH)', 'SV Pre-Crash Movement', 'SV Contact Area'# Note: This should be numerical but included here for encoding\n",
    "]\n",
    "\n",
    "datetime_cols = ['Incident Date', 'Incident Time (24:00)']\n",
    "numerical_cols = []  # Add any truly numerical columns here\n",
    "index_cols = ['Report ID', 'Report Version']\n",
    "\n",
    "US_categorical_cols = [\n",
    "    'Highest Injury Severity Alleged', 'Roadway Type', 'Lighting',\n",
    "    'Weather', 'Roadway Surface', 'City', 'State',\n",
    "    'Make', 'Model', 'Model Year', 'ADS Equipped?', 'Automation System Engaged?',\n",
    "    'Posted Speed Limit (MPH)', 'SV Pre-Crash Movement', 'SV Contact Area'# Note: This should be numerical but included here for encoding\n",
    "]\n",
    "\n",
    "UK_categorical_cols = [\n",
    "    'Highest Injury Severity Alleged', 'Roadway Type', 'Lighting',\n",
    "    'Weather', 'Roadway Surface', 'City', 'State',\n",
    "    'Make', 'Model', 'Model Year', \n",
    "    'Posted Speed Limit (MPH)','SV Pre-Crash Movement', 'SV Contact Area' # Note: This should be numerical but included here for encoding\n",
    "]\n",
    "\n",
    "# === Define columns ===\n",
    "categorical_columns = [\n",
    "    'Highest Injury Severity Alleged', 'Roadway Type', 'Lighting',\n",
    "    'Weather', 'Roadway Surface', 'City', 'State','Country','Report ID', 'Report Version',\n",
    "    'Make', 'Model', 'Model Year', 'ADS Equipped?', 'Automation System Engaged?',\n",
    "    'Posted Speed Limit (MPH)', 'SV Pre-Crash Movement', 'SV Contact Area','Crash With'\n",
    "]\n",
    "\n",
    "datetime_cols = ['Incident Date', 'Incident Time (24:00)']\n",
    "numerical_cols = []  # Add real numerical columns here if any\n",
    "\n",
    "\n",
    "def encode_data(df, categorical_cols, datetime_cols, numerical_cols, index_cols):\n",
    "    \"\"\"\n",
    "    Encodes categorical columns using LabelEncoder and datetime columns as numeric values.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    import pandas as pd\n",
    "\n",
    "    df_encoded = df.copy()\n",
    "    encoders = {}\n",
    "\n",
    "    # Encode categorical columns\n",
    "    for col in categorical_cols:\n",
    "        if col in df_encoded.columns:\n",
    "            df_encoded[col] = df_encoded[col].astype(str).fillna(\"Unknown\")\n",
    "            le = LabelEncoder()\n",
    "            df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "            encoders[col] = le\n",
    "\n",
    "    # Convert datetime columns to numeric formats\n",
    "    if 'Incident Date' in datetime_cols and 'Incident Date' in df_encoded.columns:\n",
    "        df_encoded['Incident Date'] = pd.to_datetime(df_encoded['Incident Date'], errors='coerce')\n",
    "        df_encoded['Incident_Year'] = df_encoded['Incident Date'].dt.year\n",
    "        df_encoded['Incident_Month'] = df_encoded['Incident Date'].dt.month\n",
    "        df_encoded['Incident_DayOfWeek'] = df_encoded['Incident Date'].dt.dayofweek\n",
    "        df_encoded.drop(columns=['Incident Date'], inplace=True)\n",
    "\n",
    "    if 'Incident Time (24:00)' in datetime_cols and 'Incident Time (24:00)' in df_encoded.columns:\n",
    "        time_parsed = pd.to_datetime(df_encoded['Incident Time (24:00)'], format='%H:%M:%S', errors='coerce')\n",
    "        df_encoded['Incident Time (24:00)'] = (\n",
    "            time_parsed.dt.hour.fillna(0).astype(int) * 3600 +\n",
    "            time_parsed.dt.minute.fillna(0).astype(int) * 60 +\n",
    "            time_parsed.dt.second.fillna(0).astype(int)\n",
    "        )\n",
    "\n",
    "\n",
    "    return df_encoded, encoders\n",
    "\n",
    "\n",
    "# === Encode both datasets ===\n",
    "US_encoded_df, us_encoders = encode_data(\n",
    "    df=US_data, # already imputed\n",
    "    categorical_cols=categorical_columns,\n",
    "    datetime_cols=datetime_cols,\n",
    "    numerical_cols=numerical_cols,\n",
    "    index_cols=index_cols\n",
    ")\n",
    "\n",
    "UK_encoded_df, uk_encoders = encode_data(\n",
    "    df=UK_data,\n",
    "    categorical_cols=categorical_columns,\n",
    "    datetime_cols=datetime_cols,\n",
    "    numerical_cols=numerical_cols,\n",
    "    index_cols=index_cols\n",
    ")\n",
    "\n",
    "# === Preview ===\n",
    "print(\"US Data Shape:\", US_encoded_df.shape)\n",
    "print(\"UK Data Shape:\", UK_encoded_df.shape)\n",
    "print(\"\\nUK Preview:\")\n",
    "print(tabulate(UK_encoded_df.head(), headers='keys', tablefmt='pretty'))\n",
    "\n",
    "print(\"\\nUS Preview:\")\n",
    "print(tabulate(US_encoded_df.head(), headers='keys', tablefmt='pretty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "785d138b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US Data Shape: (4159, 23)\n",
      "UK Data Shape: (229782, 23)\n",
      "\n",
      "UK Preview:\n",
      "+---+---------------------+--------------------+---------------------+---------------------+--------------------+---------------------+----------------------------+-----------------------+---------------------+---------------------+---------------------+----------------------+--------------------------+---------------------+---------------------+---------------------------------+-----------------------+---------------------+---------------------+---------+--------------------+---------------------+----------------------+\n",
      "|   |      Report ID      |   Report Version   |        Make         |        Model        |     Model Year     |    ADS Equipped?    | Automation System Engaged? | Incident Time (24:00) |        City         |        State        |    Roadway Type     |   Roadway Surface    | Posted Speed Limit (MPH) |      Lighting       |     Crash With      | Highest Injury Severity Alleged | SV Pre-Crash Movement |       Weather       |   SV Contact Area   | Country |   Incident_Year    |   Incident_Month    |  Incident_DayOfWeek  |\n",
      "+---+---------------------+--------------------+---------------------+---------------------+--------------------+---------------------+----------------------------+-----------------------+---------------------+---------------------+---------------------+----------------------+--------------------------+---------------------+---------------------+---------------------------------+-----------------------+---------------------+---------------------+---------+--------------------+---------------------+----------------------+\n",
      "| 0 | -1.731634399146567  |  49008.674597686   | 0.4215937278303381  |  12.20063552862261  | 19.296973948299506 | -0.8637197200642555 |     -0.858638350067818     |  -1.7003392075550472  | 0.05533509848334648 | -0.8820602276552807 | 0.7719130765845582  | -0.33618318221422844 |   -1.7099604346230368    | -1.5609534448212106 | -1.1770434404010701 |      -0.36713558836697674       |  0.6798280133871584   | -0.6740655117723975 | -1.3476435956682802 |   1.0   | -4.921230625440832 | -1.6288248628201223 | -0.9136388037875457  |\n",
      "| 1 | -1.7308014821676607 | 49010.34949973575  | -1.4281113537990473 |  4.940427912869666  | 19.296973948299506 | -0.8637197200642555 |     -0.858638350067818     |  -1.2766354125398125  | 3.4205550604786126  | -0.8820602276552807 | -0.9318122827902044 | -0.33618318221422844 |    -1.490218578004658    | -1.5609534448212106 | -0.4539527951088914 |        2.723789334746902        |  -2.779348027592612   | -0.6740655117723975 | -1.3476435956682802 |   1.0   | -4.921230625440832 | -1.6288248628201223 | -0.9136388037875457  |\n",
      "| 2 | -1.7299685651887542 | 49012.02440178549  | -0.6994396549753501 | 0.27071691118819324 | 20.72809143633986  | -0.8637197200642555 |     -0.858638350067818     |  0.31406969217452557  |  7.606030412726155  | -0.8820602276552807 | 0.7719130765845582  | -0.33618318221422844 |   -1.9297022912414157    | 0.7018577705888329  | -1.1770434404010701 |      -0.36713558836697674       |  -2.779348027592612   | -0.6740655117723975 | -1.3476435956682802 |   1.0   | -4.921230625440832 | -1.6288248628201223 | -0.41912763968650424 |\n",
      "| 3 | -1.7291356482098477 | 49013.699303835245 |  0.533697066110907  | -0.2746507970373801 | 14.52658232149833  | -0.8637197200642555 |     -0.858638350067818     |  1.2171640666927115   |  2.282518608439858  | -0.8820602276552807 | 0.7719130765845582  | -0.33618318221422844 |   -1.9297022912414157    | -1.5609534448212106 | -1.1770434404010701 |      -0.36713558836697674       |  -2.779348027592612   | -0.6740655117723975 | -1.3476435956682802 |   1.0   | -4.921230625440832 | -1.6288248628201223 | -0.9136388037875457  |\n",
      "| 4 | -1.7283027312309411 |  49015.374205885   | -2.2688863909033135 | -1.1267878411398384 | 20.25105227365974  | -0.8637197200642555 |     -0.858638350067818     |  -0.6713442768037631  |  7.424053223498001  | -0.8820602276552807 | -0.9318122827902044 | -0.33618318221422844 |   -1.7099604346230368    | 0.7018577705888329  | -1.1770434404010701 |        2.723789334746902        |  -2.779348027592612   | -0.6740655117723975 | -1.3476435956682802 |   1.0   | -4.921230625440832 | -1.6288248628201223 | 0.07538352441453726  |\n",
      "+---+---------------------+--------------------+---------------------+---------------------+--------------------+---------------------+----------------------------+-----------------------+---------------------+---------------------+---------------------+----------------------+--------------------------+---------------------+---------------------+---------------------------------+-----------------------+---------------------+---------------------+---------+--------------------+---------------------+----------------------+\n",
      "\n",
      "US Preview:\n",
      "+---+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+----------------------------+-----------------------+----------------------+---------------------+---------------------+----------------------+--------------------------+---------------------+---------------------+---------------------------------+-----------------------+---------------------+---------------------+---------+---------------------+---------------------+----------------------+\n",
      "|   |      Report ID      |   Report Version    |        Make         |        Model        |     Model Year      |    ADS Equipped?    | Automation System Engaged? | Incident Time (24:00) |         City         |        State        |    Roadway Type     |   Roadway Surface    | Posted Speed Limit (MPH) |      Lighting       |     Crash With      | Highest Injury Severity Alleged | SV Pre-Crash Movement |       Weather       |   SV Contact Area   | Country |    Incident_Year    |   Incident_Month    |  Incident_DayOfWeek  |\n",
      "+---+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+----------------------------+-----------------------+----------------------+---------------------+---------------------+----------------------+--------------------------+---------------------+---------------------+---------------------------------+-----------------------+---------------------+---------------------+---------+---------------------+---------------------+----------------------+\n",
      "| 0 | -1.731634399146567  |  4.390426099152441  | -1.0357496698170565 | 0.06620402060360325 | 0.21540744109479712 | -0.8637197200642555 |     -0.858638350067818     |   1.016207409628343   | -0.18367971721930357 | 1.4240676396232126  |  1.339821529709479  | -0.33618318221422844 |   -1.0507348647679005    | -0.9952506409686998 | -0.4539527951088914 |      -0.36713558836697674       |  0.36535746420717924  | 1.9811772168952408  | 0.4478234778131008  |   0.0   | 0.6117447756518682  | 1.2406611607603963  |  0.5698946885155788  |\n",
      "| 1 | -1.7308014821676607 | -0.6342800500976971 | -1.0357496698170565 | 0.06620402060360325 | 0.21540744109479712 |  1.15778298997921   |     1.1646346799220146     |   1.016207409628343   | -0.1809636397681371  | 1.4240676396232126  |  1.339821529709479  | -0.33618318221422844 |   -1.0507348647679005    | -0.9952506409686998 | -0.4539527951088914 |      -0.36713558836697674       |  0.36535746420717924  | 1.9811772168952408  | 0.4478234778131008  |   0.0   | -3.0769054917432657 | 1.2406611607603963  | -0.41912763968650424 |\n",
      "| 2 | -1.7299685651887542 | -0.6342800500976971 | -1.0357496698170565 | 0.13437498413179993 | -0.2616317215853206 |  1.15778298997921   |     1.1646346799220146     |  0.4230220966070145   |  1.2313966348384313  | -0.7574046672618486 | -0.9318122827902044 | -0.33618318221422844 |    0.9269418447975084    | 0.7018577705888329  | -0.4539527951088914 |      -0.36713558836697674       |  0.36535746420717924  | -0.6740655117723975 | 0.4478234778131008  |   0.0   | -2.154742924894482  | -0.7679790557459666 |  1.5589170167176618  |\n",
      "| 3 | -1.7291356482098477 | -0.6342800500976971 | -1.0357496698170565 | 0.06620402060360325 | 0.21540744109479712 | -0.8637197200642555 |     -0.858638350067818     |  0.31891202126041396  |  1.5600420064295752  |  0.302167596082324  | -0.9318122827902044 | -0.33618318221422844 |    0.9269418447975084    | 0.7018577705888329  | 0.26913785018328723 |      -0.36713558836697674       |  0.36535746420717924  | -0.6740655117723975 | 0.4478234778131008  |   0.0   | 0.6117447756518682  | 0.9537125584023445  | -0.41912763968650424 |\n",
      "| 4 | -1.7283027312309411 |  1.040621999652349  | -1.0357496698170565 | 0.13437498413179993 | 0.21540744109479712 | -0.8637197200642555 |     -0.858638350067818     |  -0.6713442768037631  |  0.0906441053485107  |  1.985017661393657  | -0.9318122827902044 | -0.33618318221422844 |    1.146683701415887     | 0.7018577705888329  | -0.4539527951088914 |      -0.36713558836697674       |  0.36535746420717924  | -0.6740655117723975 | -0.7491545711744866 |   0.0   | 0.6117447756518682  | 1.5276097631184482  | -1.4081499678885872  |\n",
      "+---+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+----------------------------+-----------------------+----------------------+---------------------+---------------------+----------------------+--------------------------+---------------------+---------------------+---------------------------------+-----------------------+---------------------+---------------------+---------+---------------------+---------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from tabulate import tabulate\n",
    "\n",
    "def scale_and_preview(df_US, df_UK, scaler_type='standard'):\n",
    "    \"\"\"\n",
    "    Scale two dataframes (e.g., US and UK) using StandardScaler or MinMaxScaler,\n",
    "    and print their shapes and head previews.\n",
    "    \"\"\"\n",
    "    # Choose the scaler\n",
    "    if scaler_type == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler_type == 'minmax':\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        raise ValueError(\"scaler_type must be 'standard' or 'minmax'\")\n",
    "    \n",
    "    # Scale both dataframes\n",
    "    US_scaled = scaler.fit_transform(df_US)\n",
    "    UK_scaled = scaler.transform(df_UK)  # assume same features\n",
    "\n",
    "    # Convert back to DataFrames with original column names\n",
    "    US_scaled_df = pd.DataFrame(US_scaled, columns=df_US.columns)\n",
    "    UK_scaled_df = pd.DataFrame(UK_scaled, columns=df_UK.columns)\n",
    "\n",
    "    # Preview\n",
    "    print(\"US Data Shape:\", US_scaled_df.shape)\n",
    "    print(\"UK Data Shape:\", UK_scaled_df.shape)\n",
    "\n",
    "    print(\"\\nUK Preview:\")\n",
    "    print(tabulate(UK_scaled_df.head(), headers='keys', tablefmt='pretty'))\n",
    "\n",
    "    print(\"\\nUS Preview:\")\n",
    "    print(tabulate(US_scaled_df.head(), headers='keys', tablefmt='pretty'))\n",
    "\n",
    "    return US_scaled_df, UK_scaled_df\n",
    "US_scaled_df, UK_scaled_df = scale_and_preview(US_encoded_df, UK_encoded_df, scaler_type='standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96e1bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Shared Evaluation Function\n",
    "def evaluate_model(y_true, y_pred, y_proba, model_name, dataset_label):\n",
    "    print(f\"\\nðŸ“Š {model_name} Evaluation on {dataset_label} Dataset\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_true, y_pred))\n",
    "    print(\"F1 Score:\", f1_score(y_true, y_pred))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_true, y_proba))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95f3600d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš¦ Running Logistic Regression on UK (Human Driving)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoef\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m â†’ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirection\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m severity risk\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# âœ… Run Each Model Separately\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# ðŸ”¹ Logistic Regression\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mrun_logistic_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUK_scaled_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUK (Human Driving)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m run_logistic_regression(US_scaled_df, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUS (Autonomous Driving)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[39], line 10\u001b[0m, in \u001b[0;36mrun_logistic_regression\u001b[0;34m(df, label)\u001b[0m\n\u001b[1;32m      7\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     12\u001b[0m y_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1231\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m   1222\u001b[0m X, y \u001b[38;5;241m=\u001b[39m validate_data(\n\u001b[1;32m   1223\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1224\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1229\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39msolver \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1230\u001b[0m )\n\u001b[0;32m-> 1231\u001b[0m \u001b[43mcheck_classification_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;66;03m# TODO(1.7) remove multi_class\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/multiclass.py:222\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    214\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m ]:\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Maybe you are trying to fit a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier, which expects discrete classes on a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression target with continuous values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    226\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values."
     ]
    }
   ],
   "source": [
    "\n",
    "# ðŸš¦ Logistic Regression\n",
    "def run_logistic_regression(df, label=\"Dataset\"):\n",
    "    print(f\"\\nðŸš¦ Running Logistic Regression on {label}\")\n",
    "    df = df.dropna()\n",
    "    X = pd.get_dummies(df.drop(\"Highest Injury Severity Alleged\", axis=1), drop_first=True)\n",
    "    y = df[\"Highest Injury Severity Alleged\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    evaluate_model(y_test, y_pred, y_proba, \"Logistic Regression\", label)\n",
    "\n",
    "    print(\"\\nðŸ§  Coefficients (Logistic Regression):\")\n",
    "    for feature, coef in zip(X_train.columns, model.coef_[0]):\n",
    "        direction = \"â†‘ increases\" if coef > 0 else \"â†“ decreases\"\n",
    "        print(f\"{feature}: {coef:.3f} â†’ {direction} severity risk\")\n",
    "\n",
    "# âœ… Run Each Model Separately\n",
    "# ðŸ”¹ Logistic Regression\n",
    "run_logistic_regression(UK_scaled_df, label=\"UK (Human Driving)\")\n",
    "run_logistic_regression(US_scaled_df, label=\"US (Autonomous Driving)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ccf7fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒ² Running Random Forest on UK (Human Driving)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     evaluate_model(y_test, y_pred, y_proba, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m\"\u001b[39m, label)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# ðŸ”¹ Random Forest\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mrun_random_forest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUK_scaled_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUK (Human Driving)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m run_random_forest(US_scaled_df, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUS (Autonomous Driving)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[40], line 10\u001b[0m, in \u001b[0;36mrun_random_forest\u001b[0;34m(df, label)\u001b[0m\n\u001b[1;32m      7\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     12\u001b[0m y_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:419\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    413\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum of y is not strictly positive which \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    414\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis necessary for Poisson regression.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    415\u001b[0m         )\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_samples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 419\u001b[0m y, expanded_class_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_y_class_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(y, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m!=\u001b[39m DOUBLE \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m y\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mcontiguous:\n\u001b[1;32m    422\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(y, dtype\u001b[38;5;241m=\u001b[39mDOUBLE)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:831\u001b[0m, in \u001b[0;36mForestClassifier._validate_y_class_weight\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_y_class_weight\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[0;32m--> 831\u001b[0m     \u001b[43mcheck_classification_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(y)\n\u001b[1;32m    834\u001b[0m     expanded_class_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/multiclass.py:222\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    214\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m ]:\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Maybe you are trying to fit a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier, which expects discrete classes on a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression target with continuous values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    226\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values."
     ]
    }
   ],
   "source": [
    "# ðŸŒ² Random Forest\n",
    "def run_random_forest(df, label=\"Dataset\"):\n",
    "    print(f\"\\nðŸŒ² Running Random Forest on {label}\")\n",
    "    df = df.dropna()\n",
    "    X = pd.get_dummies(df.drop(\"Highest Injury Severity Alleged\", axis=1), drop_first=True)\n",
    "    y = df[\"Highest Injury Severity Alleged\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    evaluate_model(y_test, y_pred, y_proba, \"Random Forest\", label)\n",
    "\n",
    "# ðŸ”¹ Random Forest\n",
    "run_random_forest(UK_scaled_df, label=\"UK (Human Driving)\")\n",
    "run_random_forest(US_scaled_df, label=\"US (Autonomous Driving)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4fa2983b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš¡ Running XGBoost on UK (Human Driving)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1], got [-0.36713559  2.72378933]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# ðŸ”¹ XGBoost\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mrun_xgboost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUK_scaled_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUK (Human Driving)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m run_xgboost(US_scaled_df, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUS (Autonomous Driving)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[41], line 10\u001b[0m, in \u001b[0;36mrun_xgboost\u001b[0;34m(df, label)\u001b[0m\n\u001b[1;32m      7\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBClassifier(use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     12\u001b[0m y_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/xgboost/sklearn.py:1640\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1635\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1637\u001b[0m     classes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1638\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[1;32m   1639\u001b[0m ):\n\u001b[0;32m-> 1640\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1641\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1642\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1643\u001b[0m     )\n\u001b[1;32m   1645\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1], got [-0.36713559  2.72378933]"
     ]
    }
   ],
   "source": [
    "# âš¡ XGBoost + SHAP\n",
    "def run_xgboost(df, label=\"Dataset\"):\n",
    "    print(f\"\\nâš¡ Running XGBoost on {label}\")\n",
    "    df = df.dropna()\n",
    "    X = pd.get_dummies(df.drop(\"Highest Injury Severity Alleged\", axis=1), drop_first=True)\n",
    "    y = df[\"Highest Injury Severity Alleged\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    evaluate_model(y_test, y_pred, y_proba, \"XGBoost\", label)\n",
    "\n",
    "    # SHAP Explanation\n",
    "    explainer = shap.Explainer(model, X_train)\n",
    "    shap_values = explainer(X_test)\n",
    "\n",
    "    print(f\"\\nðŸ“ˆ SHAP Summary Plot for {label}\")\n",
    "    shap.summary_plot(shap_values, X_test, show=False)\n",
    "    plt.title(f\"SHAP Feature Importance - {label}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ðŸ”¹ XGBoost\n",
    "run_xgboost(UK_scaled_df, label=\"UK (Human Driving)\")\n",
    "run_xgboost(US_scaled_df, label=\"US (Autonomous Driving)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
