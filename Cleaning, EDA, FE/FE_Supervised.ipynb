{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a3e4c96",
   "metadata": {},
   "source": [
    "# **<span style=\"color:lightgreen;\">Scaling, Imputing and Heatmaps with Supervised Learning</span>**\n",
    "\n",
    "## **Purpose of Notebook**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93426666",
   "metadata": {},
   "source": [
    "- Load datasets\n",
    "- Sample to balance size\n",
    "- Dummy fill NaNs\n",
    "- Encode categoricals\n",
    "- Scale numericals\n",
    "- Supervised imputation\n",
    "- Correlation heatmap\n",
    "- Feature selection (top 10)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5686d8",
   "metadata": {},
   "source": [
    "**Load needed Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c4df137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# Models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, mean_squared_error\n",
    ")\n",
    "\n",
    "# Clustering (if needed)\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f814f612",
   "metadata": {},
   "source": [
    "**Load in the cleaned datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cabc936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US Dataset Shape: (4372, 21)\n",
      "UK Dataset Shape: (232365, 21)\n"
     ]
    }
   ],
   "source": [
    "# Display all columns of a pandas DataFrame when printed\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# US Dataset\n",
    "US_data = pd.read_csv('/Users/mahnooriqbal/COMP702 Project/ML-AI-Risk-Analysis-AV-Data-/Datasets/US-cleaned_data.csv')\n",
    "\n",
    "\n",
    "# UK Dataset\n",
    "UK_data = pd.read_csv('/Users/mahnooriqbal/COMP702 Project/ML-AI-Risk-Analysis-AV-Data-/Datasets/UK-cleaned_data.csv')\n",
    "\n",
    "print(\"US Dataset Shape:\", US_data.shape)\n",
    "print(\"UK Dataset Shape:\", UK_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d5d594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical columns\n",
    "numerical_columns = ['Posted Speed Limit (MPH)']\n",
    "categorical_columns = [\n",
    "    'Make', 'Model', 'Model Year', 'ADS Equipped?',\n",
    "    'Automation System Engaged?', 'City', 'State', 'Roadway Type', 'Roadway Surface',\n",
    "    'Lighting', 'Crash With', 'Highest Injury Severity Alleged',\n",
    "    'SV Pre-Crash Movement', 'SV Contact Area', 'Weather', 'Country'\n",
    "]\n",
    "datetime_columns = ['Incident Date', 'Incident Time (24:00)']\n",
    "indexing_columns = ['Report ID', 'Report Version']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866dda7b",
   "metadata": {},
   "source": [
    "**Fill in Missing Data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed80aee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in UK Dataset:\n",
      "+---------+------------------------------+\n",
      "| Feature | Percentage of Missing Values |\n",
      "+---------+------------------------------+\n",
      "|  Model  |            1.11 %            |\n",
      "+---------+------------------------------+\n",
      "Missing Values in US Dataset:\n",
      "+--------------------------+------------------------------+\n",
      "|         Feature          | Percentage of Missing Values |\n",
      "+--------------------------+------------------------------+\n",
      "|        Model Year        |            0.32 %            |\n",
      "|      Incident Date       |            0.43 %            |\n",
      "|  Incident Time (24:00)   |            4.6 %             |\n",
      "| Posted Speed Limit (MPH) |           11.34 %            |\n",
      "+--------------------------+------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "# Function to calculate and display missing values\n",
    "def display_missing_values(dataset, dataset_name):\n",
    "    features_with_na = [feature for feature in dataset.columns if dataset[feature].isnull().sum() > 0]\n",
    "    \n",
    "    # Create a PrettyTable\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Feature\", \"Percentage of Missing Values\"]\n",
    "    \n",
    "    for feature in features_with_na:\n",
    "        missing_percentage = np.round(dataset[feature].isnull().mean() * 100, 2)\n",
    "        table.add_row([feature, f\"{missing_percentage} %\"])\n",
    "    \n",
    "    print(f\"Missing Values in {dataset_name}:\")\n",
    "    print(table)\n",
    "\n",
    "# Check missing values for both datasets\n",
    "display_missing_values(UK_data, \"UK Dataset\")\n",
    "display_missing_values(US_data, \"US Dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48494d59",
   "metadata": {},
   "source": [
    "# **Encode Catergorical and Time Series Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a516542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Make a copy\n",
    "data_copy = US_data.copy()\n",
    "\n",
    "# STEP 2: Drop index and datetime columns\n",
    "data_copy.drop(columns=indexing_columns, inplace=True)\n",
    "data_copy.drop(columns=datetime_columns, inplace=True)\n",
    "\n",
    "# STEP 3: Encode all categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    if col in data_copy.columns:\n",
    "        le = LabelEncoder()\n",
    "        data_copy[col] = le.fit_transform(data_copy[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# STEP 4: Combine date and time into a single datetime column\n",
    "US_data['incident_datetime'] = pd.to_datetime(\n",
    "    US_data['Incident Date'].astype(str) + ' ' + US_data['Incident Time (24:00)'],\n",
    "    format='%Y-%m-%d %H:%M:%S',\n",
    "    errors='coerce'\n",
    ")\n",
    "# STEP 5: Extract datetime features\n",
    "data_copy['incident_year'] = US_data['incident_datetime'].dt.year\n",
    "data_copy['incident_month'] = US_data['incident_datetime'].dt.month\n",
    "data_copy['incident_day'] = US_data['incident_datetime'].dt.day\n",
    "data_copy['incident_hour'] = US_data['incident_datetime'].dt.hour\n",
    "data_copy['incident_minute'] = US_data['incident_datetime'].dt.minute\n",
    "data_copy['incident_dayofweek'] = US_data['incident_datetime'].dt.dayofweek\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da3c6c4",
   "metadata": {},
   "source": [
    "**Scale Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b535dcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Make     Model  Model Year  ADS Equipped?  Automation System Engaged?  \\\n",
      "0  0.397059  0.493151    0.615385            0.0                         1.0   \n",
      "1  0.397059  0.493151    0.615385            1.0                         1.0   \n",
      "2  0.397059  0.502283    0.538462            1.0                         1.0   \n",
      "3  0.397059  0.493151    0.615385            0.0                         0.0   \n",
      "4  0.397059  0.502283    0.615385            0.0                         1.0   \n",
      "\n",
      "       City     State  Roadway Type  Roadway Surface  \\\n",
      "0  0.518072  0.725490      0.571429              0.0   \n",
      "1  0.518781  0.725490      0.571429              0.0   \n",
      "2  0.907867  0.039216      0.000000              0.0   \n",
      "3  0.555634  0.372549      0.000000              0.0   \n",
      "4  0.593905  0.901961      0.000000              0.0   \n",
      "\n",
      "   Posted Speed Limit (MPH)  Lighting  Crash With  \\\n",
      "0                    0.3125  0.166667    0.533333   \n",
      "1                    0.3125  0.166667    0.533333   \n",
      "2                    0.8125  0.666667    0.533333   \n",
      "3                    0.8125  0.666667    0.666667   \n",
      "4                    0.8750  0.666667    0.533333   \n",
      "\n",
      "   Highest Injury Severity Alleged  SV Pre-Crash Movement  Weather  \\\n",
      "0                              1.0                 0.8125    0.625   \n",
      "1                              1.0                 0.8125    0.625   \n",
      "2                              0.6                 0.8125    0.000   \n",
      "3                              1.0                 0.8125    0.000   \n",
      "4                              0.6                 0.8125    0.000   \n",
      "\n",
      "   SV Contact Area  Country  incident_year  incident_month  incident_day  \\\n",
      "0         0.727273      0.0           1.00        0.909091          0.80   \n",
      "1         0.727273      0.0           0.00        0.909091          0.00   \n",
      "2         0.727273      0.0           0.25        0.272727          0.00   \n",
      "3         0.727273      0.0           1.00        0.818182          0.85   \n",
      "4         0.272727      0.0           1.00        1.000000          0.85   \n",
      "\n",
      "   incident_hour  incident_minute  incident_dayofweek  \n",
      "0       0.826087         0.966102            0.666667  \n",
      "1       0.826087         0.966102            0.666667  \n",
      "2       0.652174         0.881356            0.166667  \n",
      "3       0.652174         0.152542            0.333333  \n",
      "4       0.347826         0.338983            0.000000  \n",
      "Missing Values in UK Dataset:\n",
      "+---------+------------------------------+\n",
      "| Feature | Percentage of Missing Values |\n",
      "+---------+------------------------------+\n",
      "|  Model  |            1.11 %            |\n",
      "+---------+------------------------------+\n",
      "Missing Values in US Dataset:\n",
      "+--------------------------+------------------------------+\n",
      "|         Feature          | Percentage of Missing Values |\n",
      "+--------------------------+------------------------------+\n",
      "|        Model Year        |            0.32 %            |\n",
      "|      Incident Date       |            0.43 %            |\n",
      "|  Incident Time (24:00)   |            4.6 %             |\n",
      "| Posted Speed Limit (MPH) |           11.34 %            |\n",
      "|    incident_datetime     |            4.6 %             |\n",
      "+--------------------------+------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data_copy_scaled = data_copy.copy()\n",
    "data_copy_scaled[data_copy.columns] = scaler.fit_transform(data_copy)\n",
    "print(data_copy_scaled.head())\n",
    "\n",
    "# Check missing values for both datasets\n",
    "display_missing_values(UK_data, \"UK Dataset\")\n",
    "display_missing_values(US_data, \"US Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d89bba8",
   "metadata": {},
   "source": [
    "# **Imputing Methods**\n",
    "- mean/median/mode\n",
    "- k Means\n",
    "- random \n",
    "- regression model\n",
    "- k nearest neighbours\n",
    "- last occured carried forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57173914",
   "metadata": {},
   "source": [
    "### **Mean, Median, Mode**\n",
    "- Best for: Small amounts of missing numerical data.\n",
    "- How it works: Replaces missing values with the mean (for normally distributed data), median (for skewed data), or mode (for categorical variables).\n",
    "- Limitations: Can distort distributions and underestimate variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91636d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+------------------+---------------+---------+--------------+-----------------+------------------------------+-----------------+-------------------------+--------------------+---------+-------------------+-------------------+----------------------------+--------------------+--------------------+-----------------------------------+-------------------------+-----------+--------------------------------+-----------+---------------------+\n",
      "|    | Report ID   |   Report Version | Make          | Model   |   Model Year | ADS Equipped?   | Automation System Engaged?   | Incident Date   | Incident Time (24:00)   | City               | State   | Roadway Type      | Roadway Surface   |   Posted Speed Limit (MPH) | Lighting           | Crash With         | Highest Injury Severity Alleged   | SV Pre-Crash Movement   | Weather   | SV Contact Area                | Country   | incident_datetime   |\n",
      "|----+-------------+------------------+---------------+---------+--------------+-----------------+------------------------------+-----------------+-------------------------+--------------------+---------+-------------------+-------------------+----------------------------+--------------------+--------------------+-----------------------------------+-------------------------+-----------+--------------------------------+-----------+---------------------|\n",
      "|  0 | 10003-2800  |                4 | International | LT      |         2022 | No              | Unknown, see Narrative       | 2025-11-21      | 19:57:00                | Maxatawny Tounship | PA      | Street            | Dry               |                         25 | Dark - Not Lighted | Other Fixed Object | Unknown                           | Proceeding Straight     | Missing   | Front                          | US        | 2025-11-21 19:57:00 |\n",
      "|  1 | 10003-2932  |                1 | International | LT      |         2022 | Yes             | Unknown, see Narrative       | 2021-11-05      | 19:57:00                | Maxatawny Township | PA      | Street            | Dry               |                         25 | Dark - Not Lighted | Other Fixed Object | Unknown                           | Proceeding Straight     | Missing   | Front                          | US        | 2021-11-05 19:57:00 |\n",
      "|  2 | 10003-3210  |                1 | International | LT625   |         2021 | Yes             | Unknown, see Narrative       | 2022-04-05      | 15:52:00                | Tucson             | AZ      | Highway / Freeway | Dry               |                         65 | Daylight           | Other Fixed Object | No Injuries Reported              | Proceeding Straight     | Clear     | Front                          | US        | 2022-04-05 15:52:00 |\n",
      "|  3 | 10003-4179  |                1 | International | LT      |         2022 | No              | ADAS                         | 2025-10-22      | 15:09:00                | Missing            | MD      | Highway / Freeway | Dry               |                         65 | Daylight           | Passenger Car      | Unknown                           | Proceeding Straight     | Clear     | Front                          | US        | 2025-10-22 15:09:00 |\n",
      "|  4 | 10003-6050  |                2 | International | LT625   |         2022 | No              | Unknown, see Narrative       | 2025-12-22      | 08:20:00                | Natural Bridge     | VA      | Highway / Freeway | Dry               |                         70 | Daylight           | Other Fixed Object | No Injuries Reported              | Proceeding Straight     | Clear     | Back, Front, Nearside, Offside | US        | 2025-12-22 08:20:00 |\n",
      "+----+-------------+------------------+---------------+---------+--------------+-----------------+------------------------------+-----------------+-------------------------+--------------------+---------+-------------------+-------------------+----------------------------+--------------------+--------------------+-----------------------------------+-------------------------+-----------+--------------------------------+-----------+---------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0k/whv99z5n5rnb1xp8n07krpcw0000gn/T/ipykernel_8791/2025267401.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(data[col].mean(), inplace=True)\n",
      "/var/folders/0k/whv99z5n5rnb1xp8n07krpcw0000gn/T/ipykernel_8791/2025267401.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(data[col].mode()[0], inplace=True)\n",
      "/var/folders/0k/whv99z5n5rnb1xp8n07krpcw0000gn/T/ipykernel_8791/2025267401.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(data[col].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def mode_mean_imputation(data, numerical_columns, categorical_columns, datetime_columns):\n",
    "    \"\"\"\n",
    "    Impute missing values in numerical and categorical columns using mean for numerical and mode for categorical and datetime.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame with missing values\n",
    "    - numerical_columns: List of numerical columns to impute\n",
    "    - categorical_columns: List of categorical columns to impute\n",
    "    - datetime_columns: List of datetime columns to impute\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with imputed values\n",
    "    \"\"\"\n",
    "    # Impute numerical columns with mean\n",
    "    for col in numerical_columns:\n",
    "        if col in data.columns:\n",
    "            data[col].fillna(data[col].mean(), inplace=True)\n",
    "    \n",
    "    # Impute categorical columns with mode\n",
    "    for col in categorical_columns:\n",
    "        if col in data.columns:\n",
    "            data[col].fillna(data[col].mode()[0], inplace=True)\n",
    "    \n",
    "    # Impute datetime columns with mode\n",
    "    for col in datetime_columns:\n",
    "        if col in data.columns:\n",
    "            data[col].fillna(data[col].mode()[0], inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "mode_imputed_df = mode_mean_imputation(US_data.copy(), numerical_columns, categorical_columns, datetime_columns)\n",
    "from tabulate import tabulate\n",
    "\n",
    "print(tabulate(mode_imputed_df.head(), headers='keys', tablefmt='psql'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6fd453",
   "metadata": {},
   "source": [
    "### **Random Imputation**\n",
    "- **Best for:** Maintaining original distribution with enough data  \n",
    "- **How it works:** Replaces missing by random sampling from existing values  \n",
    "- **Limitations:** Adds randomness; may create unrealistic combinations  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a489abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+------------------+---------------+---------+--------------+-----------------+------------------------------+-----------------+-------------------------+--------------------+---------+-------------------+-------------------+----------------------------+--------------------+--------------------+-----------------------------------+-------------------------+-----------+--------------------------------+-----------+---------------------+\n",
      "|    | Report ID   |   Report Version | Make          | Model   |   Model Year | ADS Equipped?   | Automation System Engaged?   | Incident Date   | Incident Time (24:00)   | City               | State   | Roadway Type      | Roadway Surface   |   Posted Speed Limit (MPH) | Lighting           | Crash With         | Highest Injury Severity Alleged   | SV Pre-Crash Movement   | Weather   | SV Contact Area                | Country   | incident_datetime   |\n",
      "|----+-------------+------------------+---------------+---------+--------------+-----------------+------------------------------+-----------------+-------------------------+--------------------+---------+-------------------+-------------------+----------------------------+--------------------+--------------------+-----------------------------------+-------------------------+-----------+--------------------------------+-----------+---------------------|\n",
      "|  0 | 10003-2800  |                4 | International | LT      |         2022 | No              | Unknown, see Narrative       | 2025-11-21      | 19:57:00                | Maxatawny Tounship | PA      | Street            | Dry               |                         25 | Dark - Not Lighted | Other Fixed Object | Unknown                           | Proceeding Straight     | Missing   | Front                          | US        | 2025-11-21 19:57:00 |\n",
      "|  1 | 10003-2932  |                1 | International | LT      |         2022 | Yes             | Unknown, see Narrative       | 2021-11-05      | 19:57:00                | Maxatawny Township | PA      | Street            | Dry               |                         25 | Dark - Not Lighted | Other Fixed Object | Unknown                           | Proceeding Straight     | Missing   | Front                          | US        | 2021-11-05 19:57:00 |\n",
      "|  2 | 10003-3210  |                1 | International | LT625   |         2021 | Yes             | Unknown, see Narrative       | 2022-04-05      | 15:52:00                | Tucson             | AZ      | Highway / Freeway | Dry               |                         65 | Daylight           | Other Fixed Object | No Injuries Reported              | Proceeding Straight     | Clear     | Front                          | US        | 2022-04-05 15:52:00 |\n",
      "|  3 | 10003-4179  |                1 | International | LT      |         2022 | No              | ADAS                         | 2025-10-22      | 15:09:00                | Missing            | MD      | Highway / Freeway | Dry               |                         65 | Daylight           | Passenger Car      | Unknown                           | Proceeding Straight     | Clear     | Front                          | US        | 2025-10-22 15:09:00 |\n",
      "|  4 | 10003-6050  |                2 | International | LT625   |         2022 | No              | Unknown, see Narrative       | 2025-12-22      | 08:20:00                | Natural Bridge     | VA      | Highway / Freeway | Dry               |                         70 | Daylight           | Other Fixed Object | No Injuries Reported              | Proceeding Straight     | Clear     | Back, Front, Nearside, Offside | US        | 2025-12-22 08:20:00 |\n",
      "+----+-------------+------------------+---------------+---------+--------------+-----------------+------------------------------+-----------------+-------------------------+--------------------+---------+-------------------+-------------------+----------------------------+--------------------+--------------------+-----------------------------------+-------------------------+-----------+--------------------------------+-----------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_imputation(data, numerical_columns, categorical_columns, datetime_columns):\n",
    "    \"\"\"\n",
    "    Impute missing values by randomly sampling from non-missing values in each column.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame with missing values\n",
    "    - numerical_columns: List of numerical columns to impute\n",
    "    - categorical_columns: List of categorical columns to impute\n",
    "    - datetime_columns: List of datetime columns to impute\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with imputed values\n",
    "    \"\"\"\n",
    "    for col in numerical_columns + categorical_columns + datetime_columns:\n",
    "        if col in data.columns:\n",
    "            non_null_values = data[col].dropna().values\n",
    "            # Only impute if there are missing values and non-null values exist\n",
    "            if data[col].isnull().sum() > 0 and len(non_null_values) > 0:\n",
    "                # Randomly choose from non-null values to fill missing\n",
    "                random_samples = np.random.choice(non_null_values, size=data[col].isnull().sum(), replace=True)\n",
    "                data.loc[data[col].isnull(), col] = random_samples\n",
    "                \n",
    "    return data\n",
    "\n",
    "# Use your data and columns as before\n",
    "mode_imputed_df = random_imputation(US_data.copy(), numerical_columns, categorical_columns, datetime_columns)\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate(mode_imputed_df.head(), headers='keys', tablefmt='psql'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8ff8a0",
   "metadata": {},
   "source": [
    "### **Last Occurance Imputation**\n",
    "- **Best for:** Maintaining original distribution with enough data  \n",
    "- **How it works:** Replaces missing by random sampling from existing values  \n",
    "- **Limitations:** Adds randomness; may create unrealistic combinations  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e49ade2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+------------------+---------------+---------+--------------+-----------------+------------------------------+-----------------+-------------------------+--------------------+---------+-------------------+-------------------+----------------------------+--------------------+--------------------+-----------------------------------+-------------------------+-----------+--------------------------------+-----------+---------------------+\n",
      "|    | Report ID   |   Report Version | Make          | Model   |   Model Year | ADS Equipped?   | Automation System Engaged?   | Incident Date   | Incident Time (24:00)   | City               | State   | Roadway Type      | Roadway Surface   |   Posted Speed Limit (MPH) | Lighting           | Crash With         | Highest Injury Severity Alleged   | SV Pre-Crash Movement   | Weather   | SV Contact Area                | Country   | incident_datetime   |\n",
      "|----+-------------+------------------+---------------+---------+--------------+-----------------+------------------------------+-----------------+-------------------------+--------------------+---------+-------------------+-------------------+----------------------------+--------------------+--------------------+-----------------------------------+-------------------------+-----------+--------------------------------+-----------+---------------------|\n",
      "|  0 | 10003-2800  |                4 | International | LT      |         2022 | No              | Unknown, see Narrative       | 2025-11-21      | 19:57:00                | Maxatawny Tounship | PA      | Street            | Dry               |                         25 | Dark - Not Lighted | Other Fixed Object | Unknown                           | Proceeding Straight     | Missing   | Front                          | US        | 2025-11-21 19:57:00 |\n",
      "|  1 | 10003-2932  |                1 | International | LT      |         2022 | Yes             | Unknown, see Narrative       | 2021-11-05      | 19:57:00                | Maxatawny Township | PA      | Street            | Dry               |                         25 | Dark - Not Lighted | Other Fixed Object | Unknown                           | Proceeding Straight     | Missing   | Front                          | US        | 2021-11-05 19:57:00 |\n",
      "|  2 | 10003-3210  |                1 | International | LT625   |         2021 | Yes             | Unknown, see Narrative       | 2022-04-05      | 15:52:00                | Tucson             | AZ      | Highway / Freeway | Dry               |                         65 | Daylight           | Other Fixed Object | No Injuries Reported              | Proceeding Straight     | Clear     | Front                          | US        | 2022-04-05 15:52:00 |\n",
      "|  3 | 10003-4179  |                1 | International | LT      |         2022 | No              | ADAS                         | 2025-10-22      | 15:09:00                | Missing            | MD      | Highway / Freeway | Dry               |                         65 | Daylight           | Passenger Car      | Unknown                           | Proceeding Straight     | Clear     | Front                          | US        | 2025-10-22 15:09:00 |\n",
      "|  4 | 10003-6050  |                2 | International | LT625   |         2022 | No              | Unknown, see Narrative       | 2025-12-22      | 08:20:00                | Natural Bridge     | VA      | Highway / Freeway | Dry               |                         70 | Daylight           | Other Fixed Object | No Injuries Reported              | Proceeding Straight     | Clear     | Back, Front, Nearside, Offside | US        | 2025-12-22 08:20:00 |\n",
      "+----+-------------+------------------+---------------+---------+--------------+-----------------+------------------------------+-----------------+-------------------------+--------------------+---------+-------------------+-------------------+----------------------------+--------------------+--------------------+-----------------------------------+-------------------------+-----------+--------------------------------+-----------+---------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0k/whv99z5n5rnb1xp8n07krpcw0000gn/T/ipykernel_8791/643073262.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(method='bfill', inplace=True)\n",
      "/var/folders/0k/whv99z5n5rnb1xp8n07krpcw0000gn/T/ipykernel_8791/643073262.py:16: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data[col].fillna(method='bfill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def last_occurrence_imputation(data, numerical_columns, categorical_columns, datetime_columns):\n",
    "    \"\"\"\n",
    "    Impute missing values using backward fill (last occurrence) for numerical, categorical, and datetime columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame with missing values\n",
    "    - numerical_columns: List of numerical columns to impute\n",
    "    - categorical_columns: List of categorical columns to impute\n",
    "    - datetime_columns: List of datetime columns to impute\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with imputed values\n",
    "    \"\"\"\n",
    "    for col in numerical_columns + categorical_columns + datetime_columns:\n",
    "        if col in data.columns:\n",
    "            data[col].fillna(method='bfill', inplace=True)\n",
    "            \n",
    "    return data\n",
    "\n",
    "# Usage example\n",
    "mode_imputed_df = last_occurrence_imputation(US_data, numerical_columns, categorical_columns, datetime_columns)\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate(mode_imputed_df.head(), headers='keys', tablefmt='psql'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aba836",
   "metadata": {},
   "source": [
    "### Regression Imputer\n",
    "- **Best for:** When strong relationships exist between features  \n",
    "- **How it works:** Predicts missing values using regression models on other features  \n",
    "- **Limitations:** Assumes linearity; risk of overfitting if data is small  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fdfbd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------------+---------------+---------+--------------+-----------------+------------------------------+--------------------+---------+-------------------+-------------------+--------------------+--------------------+-----------------------------------+-------------------------+--------------------------------+-----------+-----------+-----------------+-------------------------+\n",
      "|    |   Posted Speed Limit (MPH) | Make          | Model   |   Model Year | ADS Equipped?   | Automation System Engaged?   | City               | State   | Roadway Type      | Roadway Surface   | Lighting           | Crash With         | Highest Injury Severity Alleged   | SV Pre-Crash Movement   | SV Contact Area                | Weather   | Country   | Incident Date   | Incident Time (24:00)   |\n",
      "|----+----------------------------+---------------+---------+--------------+-----------------+------------------------------+--------------------+---------+-------------------+-------------------+--------------------+--------------------+-----------------------------------+-------------------------+--------------------------------+-----------+-----------+-----------------+-------------------------|\n",
      "|  0 |                         25 | International | LT      |         2022 | No              | Unknown, see Narrative       | Maxatawny Tounship | PA      | Street            | Dry               | Dark - Not Lighted | Other Fixed Object | Unknown                           | Proceeding Straight     | Front                          | Missing   | US        | 2025-11-21      | 19:57:00                |\n",
      "|  1 |                         25 | International | LT      |         2022 | Yes             | Unknown, see Narrative       | Maxatawny Township | PA      | Street            | Dry               | Dark - Not Lighted | Other Fixed Object | Unknown                           | Proceeding Straight     | Front                          | Missing   | US        | 2021-11-05      | 19:57:00                |\n",
      "|  2 |                         65 | International | LT625   |         2021 | Yes             | Unknown, see Narrative       | Tucson             | AZ      | Highway / Freeway | Dry               | Daylight           | Other Fixed Object | No Injuries Reported              | Proceeding Straight     | Front                          | Clear     | US        | 2022-04-05      | 15:52:00                |\n",
      "|  3 |                         65 | International | LT      |         2022 | No              | ADAS                         | Missing            | MD      | Highway / Freeway | Dry               | Daylight           | Passenger Car      | Unknown                           | Proceeding Straight     | Front                          | Clear     | US        | 2025-10-22      | 15:09:00                |\n",
      "|  4 |                         70 | International | LT625   |         2022 | No              | Unknown, see Narrative       | Natural Bridge     | VA      | Highway / Freeway | Dry               | Daylight           | Other Fixed Object | No Injuries Reported              | Proceeding Straight     | Back, Front, Nearside, Offside | Clear     | US        | 2025-12-22      | 08:20:00                |\n",
      "+----+----------------------------+---------------+---------+--------------+-----------------+------------------------------+--------------------+---------+-------------------+-------------------+--------------------+--------------------+-----------------------------------+-------------------------+--------------------------------+-----------+-----------+-----------------+-------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0k/whv99z5n5rnb1xp8n07krpcw0000gn/T/ipykernel_8791/1622105651.py:16: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  imputed_datetime_df = regression_df[datetime_columns].fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "regression_df = US_data.copy()\n",
    "# Impute only numerical columns\n",
    "imputer = IterativeImputer(random_state=0)\n",
    "imputed_array = imputer.fit_transform(regression_df[numerical_columns])\n",
    "imputed_numerical_df = pd.DataFrame(imputed_array, columns=numerical_columns)\n",
    "\n",
    "# For categorical: fill with mode or use another strategy\n",
    "imputed_categorical_df = regression_df[categorical_columns].fillna(regression_df[categorical_columns].mode().iloc[0])\n",
    "\n",
    "# For datetime: forward fill as an example\n",
    "imputed_datetime_df = regression_df[datetime_columns].fillna(method='ffill')\n",
    "imputed_df = pd.concat([imputed_numerical_df, imputed_categorical_df, imputed_datetime_df], axis=1)\n",
    "print(tabulate(imputed_df.head(), headers='keys', tablefmt='psql'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bf4601",
   "metadata": {},
   "source": [
    "### **Comparison of Imputer Qualities**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3adf1c5",
   "metadata": {},
   "source": [
    "**Create a controlled test set with known missing values**\n",
    "- To fairly compare imputers, you need to:\n",
    "- Select a subset of your complete dataset with no missing values.\n",
    "- Introduce missing values artificially (e.g., 10% randomly) so you know the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd2624b",
   "metadata": {},
   "source": [
    "**Numerical Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bf880bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------+\n",
      "| Imputation Method   |   RMSE Score |\n",
      "|---------------------+--------------|\n",
      "| Mean                |       0.2266 |\n",
      "| Mode                |       0.3261 |\n",
      "| Regression          |       0.2266 |\n",
      "| Last Occurrence     |       0.2173 |\n",
      "| Random              |       0.318  |\n",
      "+---------------------+--------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0k/whv99z5n5rnb1xp8n07krpcw0000gn/T/ipykernel_8791/263511584.py:24: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  last_occurrence_result = data_masked.fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Drop rows with missing values to get clean baseline\n",
    "complete_data = data_copy_scaled.dropna()\n",
    "numerical_data = complete_data[numerical_columns]\n",
    "\n",
    "# 2. Randomly mask 20% of numerical values\n",
    "data_masked = numerical_data.copy()\n",
    "mask = np.random.rand(*data_masked.shape) < 0.2\n",
    "data_masked = data_masked.mask(mask)\n",
    "\n",
    "# 3. Imputation Methods\n",
    "# Mean\n",
    "mean_imp = SimpleImputer(strategy='mean')\n",
    "mean_result = pd.DataFrame(mean_imp.fit_transform(data_masked), columns=numerical_columns)\n",
    "\n",
    "# Mode\n",
    "mode_imp = SimpleImputer(strategy='most_frequent')\n",
    "mode_result = pd.DataFrame(mode_imp.fit_transform(data_masked), columns=numerical_columns)\n",
    "\n",
    "# Regression\n",
    "reg_imp = IterativeImputer(random_state=0)\n",
    "reg_result = pd.DataFrame(reg_imp.fit_transform(data_masked), columns=numerical_columns)\n",
    "\n",
    "# Last Occurrence (Forward Fill)\n",
    "last_occurrence_result = data_masked.fillna(method='ffill')\n",
    "\n",
    "# Random Imputation\n",
    "random_result = data_masked.copy()\n",
    "for col in numerical_columns:\n",
    "    missing_idx = random_result[col].isna()\n",
    "    if missing_idx.any():\n",
    "        random_sample = random_result[col].dropna().sample(missing_idx.sum(), replace=True, random_state=0).values\n",
    "        random_result.loc[missing_idx, col] = random_sample\n",
    "\n",
    "# 4. RMSE calculation at only the masked (missing) positions\n",
    "def masked_rmse(imputed, original, mask):\n",
    "    rmse_total = 0\n",
    "    n = 0\n",
    "    for i, col in enumerate(numerical_columns):\n",
    "        col_mask = mask[:, i]\n",
    "        if np.any(col_mask):\n",
    "            mse = mean_squared_error(original[col_mask, i], imputed[col_mask, i])\n",
    "            rmse_total += np.sqrt(mse)\n",
    "            n += 1\n",
    "    return rmse_total / n\n",
    "\n",
    "# 5. Convert to arrays\n",
    "original_array = numerical_data.values\n",
    "mean_array = mean_result.values\n",
    "mode_array = mode_result.values\n",
    "reg_array = reg_result.values\n",
    "last_occ_array = last_occurrence_result.values\n",
    "random_array = random_result.values\n",
    "\n",
    "# 6. Compute RMSE for each method\n",
    "rmse_scores = {\n",
    "    \"Mean\": masked_rmse(mean_array, original_array, mask),\n",
    "    \"Mode\": masked_rmse(mode_array, original_array, mask),\n",
    "    \"Regression\": masked_rmse(reg_array, original_array, mask),\n",
    "    \"Last Occurrence\": masked_rmse(last_occ_array, original_array, mask),\n",
    "    \"Random\": masked_rmse(random_array, original_array, mask)\n",
    "}\n",
    "\n",
    "# 7. Print results\n",
    "table_data = [[method, f\"{score:.4f}\"] for method, score in rmse_scores.items()]\n",
    "print(tabulate(table_data, headers=[\"Imputation Method\", \"RMSE Score\"], tablefmt=\"psql\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b6a87",
   "metadata": {},
   "source": [
    "**Catergorical Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "185be845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------------+\n",
      "| Imputation Method   |   Accuracy Score |\n",
      "|---------------------+------------------|\n",
      "| Mode                |           0.3594 |\n",
      "| Last Occurrence     |           0.3973 |\n",
      "| Random              |           0.2812 |\n",
      "+---------------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "US_data = US_data.copy()\n",
    "# ----------------------------------------------\n",
    "# 1. Drop rows with missing values in categorical columns to get clean baseline\n",
    "complete_data = US_data.dropna(subset=categorical_columns)\n",
    "\n",
    "categorical_data = complete_data[categorical_columns]\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 2. Randomly mask 20% of categorical data\n",
    "data_masked = categorical_data.copy()\n",
    "mask = np.random.rand(*data_masked.shape) < 0.2\n",
    "data_masked = data_masked.mask(mask)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 3. Imputation Methods\n",
    "\n",
    "# Mode imputation\n",
    "mode_imp = SimpleImputer(strategy='most_frequent')\n",
    "mode_result = pd.DataFrame(mode_imp.fit_transform(data_masked), columns=categorical_columns)\n",
    "\n",
    "# Forward fill imputation (with backward fill to handle NaNs at top)\n",
    "last_occurrence_result = data_masked.ffill().bfill()\n",
    "\n",
    "# Random imputation\n",
    "random_result = data_masked.copy()\n",
    "for col in categorical_columns:\n",
    "    missing_idx = random_result[col].isna()\n",
    "    if missing_idx.any():\n",
    "        random_sample = random_result[col].dropna().sample(missing_idx.sum(), replace=True, random_state=0).values\n",
    "        random_result.loc[missing_idx, col] = random_sample\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 4. Encode categorical columns as integers for accuracy calculation\n",
    "def encode_df(df):\n",
    "    encoded_df = pd.DataFrame()\n",
    "    encoders = {}\n",
    "    for col in categorical_columns:\n",
    "        encoded_df[col], encoders[col] = pd.factorize(df[col])\n",
    "    return encoded_df, encoders\n",
    "\n",
    "original_encoded, encoders = encode_df(categorical_data)\n",
    "mode_encoded, _ = encode_df(mode_result)\n",
    "last_occ_encoded, _ = encode_df(last_occurrence_result)\n",
    "random_encoded, _ = encode_df(random_result)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 5. Calculate accuracy at masked positions\n",
    "def masked_accuracy(imputed, original, mask):\n",
    "    total_acc = 0\n",
    "    n = 0\n",
    "    for i, col in enumerate(categorical_columns):\n",
    "        col_mask = mask[:, i]\n",
    "        if np.any(col_mask):\n",
    "            acc = accuracy_score(original[col_mask, i], imputed[col_mask, i])\n",
    "            total_acc += acc\n",
    "            n += 1\n",
    "    return total_acc / n\n",
    "\n",
    "# Convert mask DataFrame to numpy array if needed\n",
    "mask_array = mask.values if isinstance(mask, pd.DataFrame) else mask\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 6. Calculate accuracy scores for each imputation method\n",
    "accuracy_scores = {\n",
    "    \"Mode\": masked_accuracy(mode_encoded.values, original_encoded.values, mask_array),\n",
    "    \"Last Occurrence\": masked_accuracy(last_occ_encoded.values, original_encoded.values, mask_array),\n",
    "    \"Random\": masked_accuracy(random_encoded.values, original_encoded.values, mask_array)\n",
    "}\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 7. Print results in table format\n",
    "table_data = [[method, f\"{score:.4f}\"] for method, score in accuracy_scores.items()]\n",
    "print(tabulate(table_data, headers=[\"Imputation Method\", \"Accuracy Score\"], tablefmt=\"psql\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4de8b72",
   "metadata": {},
   "source": [
    "**Date Time columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "176f44dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------------+\n",
      "| Imputation Method   |   Accuracy Score |\n",
      "|---------------------+------------------|\n",
      "| Mode                |           0.3688 |\n",
      "| Last Occurrence     |           0.4285 |\n",
      "| Random              |           0.2828 |\n",
      "+---------------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "US_data = US_data.copy()\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 1. Drop rows with missing values in datetime columns to get clean baseline\n",
    "complete_data = US_data.dropna(subset=datetime_columns)\n",
    "\n",
    "# Select categorical columns from the cleaned data\n",
    "categorical_data = complete_data[categorical_columns]\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 2. Randomly mask 20% of categorical data\n",
    "data_masked = categorical_data.copy()\n",
    "mask = np.random.rand(*data_masked.shape) < 0.2\n",
    "data_masked = data_masked.mask(mask)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 3. Imputation Methods\n",
    "\n",
    "# Mode imputation\n",
    "mode_imp = SimpleImputer(strategy='most_frequent')\n",
    "mode_result = pd.DataFrame(mode_imp.fit_transform(data_masked), columns=categorical_columns)\n",
    "\n",
    "# Forward fill imputation (with backward fill to handle NaNs at top)\n",
    "last_occurrence_result = data_masked.ffill().bfill()\n",
    "\n",
    "# Random imputation\n",
    "random_result = data_masked.copy()\n",
    "for col in categorical_columns:\n",
    "    missing_idx = random_result[col].isna()\n",
    "    if missing_idx.any():\n",
    "        random_sample = random_result[col].dropna().sample(missing_idx.sum(), replace=True, random_state=0).values\n",
    "        random_result.loc[missing_idx, col] = random_sample\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 4. Encode categorical columns as integers for accuracy calculation\n",
    "def encode_df(df):\n",
    "    encoded_df = pd.DataFrame()\n",
    "    encoders = {}\n",
    "    for col in categorical_columns:\n",
    "        encoded_df[col], encoders[col] = pd.factorize(df[col])\n",
    "    return encoded_df, encoders\n",
    "\n",
    "original_encoded, encoders = encode_df(categorical_data)\n",
    "mode_encoded, _ = encode_df(mode_result)\n",
    "last_occ_encoded, _ = encode_df(last_occurrence_result)\n",
    "random_encoded, _ = encode_df(random_result)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 5. Calculate accuracy at masked positions\n",
    "def masked_accuracy(imputed, original, mask):\n",
    "    total_acc = 0\n",
    "    n = 0\n",
    "    for i, col in enumerate(categorical_columns):\n",
    "        col_mask = mask[:, i]\n",
    "        if np.any(col_mask):\n",
    "            acc = accuracy_score(original[col_mask, i], imputed[col_mask, i])\n",
    "            total_acc += acc\n",
    "            n += 1\n",
    "    return total_acc / n\n",
    "\n",
    "# Convert mask DataFrame to numpy array if needed\n",
    "mask_array = mask.values if isinstance(mask, pd.DataFrame) else mask\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 6. Calculate accuracy scores for each imputation method\n",
    "accuracy_scores = {\n",
    "    \"Mode\": masked_accuracy(mode_encoded.values, original_encoded.values, mask_array),\n",
    "    \"Last Occurrence\": masked_accuracy(last_occ_encoded.values, original_encoded.values, mask_array),\n",
    "    \"Random\": masked_accuracy(random_encoded.values, original_encoded.values, mask_array)\n",
    "}\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 7. Print results in table format\n",
    "table_data = [[method, f\"{score:.4f}\"] for method, score in accuracy_scores.items()]\n",
    "print(tabulate(table_data, headers=[\"Imputation Method\", \"Accuracy Score\"], tablefmt=\"psql\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfa433f",
   "metadata": {},
   "source": [
    "### **Final Imputations**\n",
    "\n",
    "For US \n",
    "- Numerical = Last Occurance\n",
    "- Catergorical = Last Occurance\n",
    "- Datetime = Last Occurance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42dc8d7",
   "metadata": {},
   "source": [
    "## **Sample Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2648857d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 21)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downsample the large dataset\n",
    "UK_data = UK_data.sample(n=4000, random_state=42)\n",
    "UK_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cef21b",
   "metadata": {},
   "source": [
    "5. Imbalanced Datasets: knn for minority"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95f1785",
   "metadata": {},
   "source": [
    "## **Outliers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7087ae5",
   "metadata": {},
   "source": [
    "**DBSCAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a7f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# If data_array is a NumPy array:\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "data_imputed = imputer.fit_transform(data_array)\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Set DBSCAN parameters\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5, metric='hamming')  # Use 'hamming' for categorical\n",
    "labels = dbscan.fit_predict(data_imputed)\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# Only calculate score if more than 1 cluster (excluding -1 for noise)\n",
    "if len(set(labels)) > 1 and len(set(labels)) != 2 or -1 not in labels:\n",
    "    score = silhouette_score(data_imputed, labels, metric='hamming')\n",
    "    print(\"Silhouette Score:\", score)\n",
    "else:\n",
    "    print(\"Too few clusters to compute silhouette score.\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "data_2d = pca.fit_transform(data_imputed)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(data_2d[:, 0], data_2d[:, 1], c=labels, cmap='tab10', s=50)\n",
    "plt.title(\"DBSCAN Clustering on Encoded Categorical Data\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c18df75",
   "metadata": {},
   "source": [
    "## **Create Heatmap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1cd605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "# STEP 5: Now you can compute correlation safely\n",
    "correlations = data_copy.corr()['Highest Injury Severity Alleged'].abs().dropna().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# Plot heatmap with correlation values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(data_copy.corr(), annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=0.5)\n",
    "plt.title(\"Feature Correlation Heatmap with Values\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# STEP 6: Top 10 most correlated (excluding the target itself)\n",
    "most_correlated_features = correlations.iloc[1:11]\n",
    "print(\"Top 10 most correlated features with 'Highest Severity Injury Alleged':\")\n",
    "print(most_correlated_features)\n",
    "\n",
    "# Filter dataset to top features\n",
    "data_filtered = data_copy[most_correlated_features.index]\n",
    "print(\"Filtered dataset shape:\", data_filtered.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
